{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "sc = SparkContext('local')\n",
    "spark = SparkSession(sc)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/08/05 20:59:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/08/05 20:59:27 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/08/05 20:59:27 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/08/05 20:59:27 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "21/08/05 20:59:27 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Read in AP Data\n",
    "df_ap = spark.read.csv('ap_data/18676_data_sample2.csv', header=True, inferSchema=True, sep=\",\", nullValue=\"\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col, trim\n",
    "# Fix Column Name Formatting\n",
    "df_ap = df_ap.toDF(*(c.replace('.', '') for c in df_ap.columns)) \n",
    "# Get null value count for each column\n",
    "# df_ap.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_ap.columns]).toPandas() "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Drop Columns with all null values\n",
    "def drop_null_columns(df):\n",
    "    \n",
    "    _df_length = df.count()\n",
    "    null_counts = df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in df.columns]).collect()[0].asDict()\n",
    "    to_drop = [k for k, v in null_counts.items() if v >= _df_length]\n",
    "    df = df.drop(*to_drop)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_ap = drop_null_columns(df_ap)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/08/05 20:59:51 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "max_columns = [c for c in df_ap.columns if 'Max' in c]\n",
    "average_columns = [c for c in df_ap.columns if 'Average' in c]\n",
    "rate_average_columns = [c for c in df_ap.columns if 'Rate Average' in c]\n",
    "std_columns = [c for c in df_ap.columns if 'Std Dev' in c]\n",
    "total_columns = [c for c in df_ap.columns if 'Total' in c]\n",
    "count_columns = [c for c in df_ap.columns if 'Count' in c]\n",
    "count_average = [c for c in df_ap.columns if 'Count Average' in c]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "columns_to_drop = total_columns + count_columns + rate_average_columns\n",
    "[columns_to_drop.remove(c) for c in count_average]\n",
    "\n",
    "df_ap = df_ap.drop(*columns_to_drop)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Temporary Filtering for Outliers because of inaccurate data\n",
    "bounds = {\n",
    "    c: dict(\n",
    "        zip([\"q1\", \"q3\"], df_ap.approxQuantile(c, [0.20, 0.80], 0))\n",
    "    )\n",
    "    for c in df_ap.columns[3:]\n",
    "}"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for c in bounds:\n",
    "    iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "    bounds[c]['lower'] = bounds[c]['q1'] - (iqr * 1.5)\n",
    "    bounds[c]['upper'] = bounds[c]['q3'] + (iqr * 1.5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "df_ap = df_ap.select(\"*\", *[F.when(F.col(c).between(bounds[c]['lower'], bounds[c]['upper']), 0).otherwise(1).alias(c+\"_out\") for c in df_ap.columns[3:]])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "from functools import reduce\n",
    "from operator import add\n",
    "\n",
    "out_columns = [c for c in df_ap.columns if 'out' in c]\n",
    "df_ap = df_ap.withColumn('sum', reduce(add, [col(x) for x in out_columns]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "df_ap = df_ap.drop(*out_columns)\n",
    "df_ap_pd = df_ap.toPandas()\n",
    "df_ap_pd.to_csv('18676_filtered_data_sample2.csv', index = False)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "21/08/06 04:40:30 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 7207105 ms exceeds timeout 120000 ms\n",
      "21/08/06 04:40:30 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit ('3.9')"
  },
  "interpreter": {
   "hash": "7812ea015bdcee6f23a998adcdd2ef97c151c0c241b7b7070987d9313e41299d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}